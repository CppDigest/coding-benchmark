# Coding-Agent Benchmarks Survey (with C++/CI Focus)

This report surveys existing **coding agent benchmarks**, especially those relevant to **C/C++ code, Clang/LLVM workflows, and CI failure fixes**. We review what each benchmark measures (task types and success signals), how widely it’s used, the cost/effort to integrate it, and how well it fits our internal evaluation needs (which include fixing CI failures in C++ projects like Boost, multi-step tool use, etc.). A comparison table is included for quick reference.

## Overview of Benchmarks

We examined a range of benchmarks covering *issue-driven bug fixes*, *program repair corpora*, *code generation tasks*, and *CI build failures*. Each provides different signals:

- **Issue→Patch Benchmarks** (e.g., SWE-Bench): Present a real GitHub issue or failing test scenario and require generating a code patch that passes all tests. These often measure if an agent can navigate a codebase and apply correct fixes.
- **C/C++ Bug Repair Suites** (e.g., Defects4C, ManyBugs): Collections of real C/C++ bugs with test cases. They measure code-fixing success in compiled languages, exposing the agent to compiler errors and low-level issues.
- **CI Failure Datasets** (e.g., BugSwarm): Real continuous-integration failures (compilation or tests) packaged for analysis. They test an agent’s ability to interpret CI logs and fix build issues in a realistic environment.
- **Code Generation Benchmarks** (e.g., HumanEval, MultiPL-E): Prompt-based coding challenges (often Python, with some C++ extensions) that measure a model’s ability to write correct code from scratch. They are widely used to gauge general coding proficiency.
- **Other Related Sets** (e.g., QuixBugs, CodeXGLUE Refinement): Smaller or synthetic collections of buggy code and fixes, often used in academic research but with limited scope (e.g., simple algorithmic bugs or snippet-level fixes).

Below, we provide a detailed comparison table of key benchmarks, followed by notes on their usage and suitability for our purposes.

## Comparison Table of Benchmarks

| **Benchmark**               | **Task Type(s)**                              | **Success Signal(s)**                        | **Cost / Effort**                         | **Fit to C++/CI Needs**                     |
|-----------------------------|----------------------------------------------|----------------------------------------------|-------------------------------------------|----------------------------------------------|
| **SWE-Bench (Verified)**    | Real GitHub *issues* → code **patch** (Python projects) ([arxivday.com](https://arxivday.com/articles?date=2024-03-23#:~:text=Arxiv%20Day%3A%20Article%20On%20The,To%20date%2C%20this%20task)) ([openreview.net](https://openreview.net/forum?id=mEV0nvHcK3#:~:text=,has%20driven%20efforts%20to%20automate)). Each task includes an issue description and repo snapshot. | **All tests pass**: previously failing tests now pass, and existing passing tests remain green (no regressions) ([openreview.net](https://openreview.net/forum?id=mEV0nvHcK3#:~:text=,has%20driven%20efforts%20to%20automate)). | ~500 issues, Python only. Provided Docker-based harness; moderate runtime (each runs a project’s test suite). | **Partial** – Strong issue-fix evaluation, but Python-centric (no compile stage). Lacks C/C++ coverage, so limited direct relevance to Clang/Boost scenarios. |
| **SWE-Bench Multilingual**  | Real *issues* → **patch** (9 languages incl. C, C++). Similar format to SWE-Bench ([arxivday.com](https://arxivday.com/articles?date=2024-03-23#:~:text=Arxiv%20Day%3A%20Article%20On%20The,To%20date%2C%20this%20task)), across multiple ecosystems. | **Tests passing** (F2P + P2P) on patched code, same as above. Measures success per language. | 300 issues total. Easy integration via SWE-Bench harness. Need language-specific tools (e.g., gcc/clang for C/C++ tasks). | **High** – Includes ~45 C/C++ tasks (with actual compile and test). Directly evaluates C/C++ bug-fixing in context, fitting our CI-fix and issue-fix needs. |
| **SWE-PolyBench** (AWS)     | Varied tasks: bug fixes, feature requests, refactoring in large repos (Java, JS/TS, Python)  . Agent gets an issue or PR description and repo. | **All tests pass** = task resolved. **Plus**: metrics like *file precision/recall* (did the agent accurately identify relevant files) , and possibly step counts. | 2,110 tasks (21 projects); heavy – full run is expensive (many large builds). A 500-task subset provided for quicker eval. Docker-based harness. | **Medium** – Great for evaluating **agent’s tool use and navigation** in complex projects. **No C++ tasks** currently (focus on Java/JS), so not directly testing Clang workflows. Useful for general agent validation, less so for C++ specifics. |
| **Defects4C** (2024)        | Real **C/C++ bug fixes** mined from OSS (248 bugs + 102 security fixes) ([openreview.net](https://openreview.net/forum?id=gXK3Y6WNVv#:~:text=OpenReview%20openreview,Submitted%20to%20ICLR)) ([openreview.net](https://openreview.net/forum?id=mEV0nvHcK3#:~:text=,has%20driven%20efforts%20to%20automate)). Usually single function/file with a bug and a test. | **Tests pass** after applying fix (bug is resolved without breaking provided tests). Sometimes categorized by bug type (memory, logic, etc.). | ~350 cases. Requires setting up each C/C++ build/test (script or container per case). New benchmark; integration may need custom harness or provided scripts. | **High** – Directly targets C/C++ faults. Ideal for assessing Clang/LLVM toolchain handling and typical C++ errors (memory leaks, UB, etc.). High relevance to Boost and low-level fixes. |
| **ManyBugs** (2015)         | Real **C program bugs** (9 programs, 185 bugs) . Multi-file codebases with failing test or output. | **Test script outcome**: bug fixed if all tests in the provided suite pass (often one specific failure to fix) . | High setup: must configure old codebases (PHP, Wireshark, etc.) and run their tests. Automation possible via BugZoo containers. | **Medium** – True C bug scenarios with realistic complexity. Good for evaluating compiler-based debugging. However, mostly C (not C++), and environment quirks may add overhead. |
| **Codeflaws** (2018)        | Competitive programming **C/C++ solutions** – each task is a wrong solution → corrected solution for an algorithmic problem. | **All test cases passed** on the corrected code (matching the online judge’s expected output). | 3,890 bug-fix pairs. Each is a single-file program; need to compile and run tests for each. Can automate in batch. | **Partial** – Large-scale C++ logic errors and fixes. Great for measuring general bug-fix ability in C++, but not representative of multi-file CI issues or build systems. |
| **BugSwarm** (2018)         | Containerized **CI failure** scenarios (mostly Java/Python, some others). Each is a failing build (with logs) + a fixed build. | **Build succeeds**: all tests pass in the failing build’s container after applying agent’s changes (i.e., matches the known fixed state)  . | 433 cases as Docker images (0.5–2 GB each). Significant compute to run containers & tests. Requires agent interaction with logs and code in-container. | **High (filtered)** – Closest analog to real CI fixes: includes compiler errors, dependency issues, flaky tests. C++ cases are few, but extremely relevant where present. Good test of end-to-end CI troubleshooting capability, albeit integration is complex. |
| **Bears** (2019)            | Travis CI **Java build failures** (251 bugs). Provides failing commit and fixed commit with tests (Maven projects). | **All tests pass** on the fixed version (no metric beyond pass/fail per bug). Often compared via patch correctness. | Java/Maven environment needed. Lighter than BugSwarm (no full containers, just code). | **Low (for C++)** – Java-specific. Demonstrates CI fix approach via commits, but doesn’t cover compiled languages. Valuable conceptually but not directly useful for C++. |
| **HumanEval** (OpenAI)      | **Code generation** from spec (164 Python coding tasks). Write a function to satisfy a docstring and examples. | **pass@1**: percentage of problems solved with one try (function passes all unit tests) . Also pass@k for multiple attempts. | Lightweight: run Python tests for 164 solutions. Often done in a secure eval harness. | **Low/Informative** – No C++ or CI aspect, but widely used metric of base coding skill. Useful to report for comparability (our model’s general coding ability), not for CI-specific insight. |
| **MultiPL-E (C++ subset)**  | **Code generation** (HumanEval problems) in **C++** . Model writes C++ code for each prompt. | **pass@1 (C++)**: fraction of translated tasks where compiled code passes tests. (Also can measure pass@k). | Moderate: ~100+ C++ test runs. Need C++17 compiler and GoogleTest or similar for each problem (harness available) . | **Medium** – Directly checks model’s C++ proficiency on small tasks. Ensures model can handle C++ syntax/stdlib. Still single-function, no CI context. Good secondary metric. |
| **APPS** (2021)             | **Coding challenges** (mostly algorithimic puzzles, free-form coding). 10k problems, varying difficulty. | **% test cases passed** or full correct output on hidden tests. Often aggregated as pass@1 or pass@100. | Heavy: running 5k evaluation problems with multiple test cases can be expensive. Often sampled in eval. | **Low** – Stresses general problem-solving, not specific to CI or C++. Might use to stress-test model’s logic in code, but not a direct measure of CI-fix skills. |
| **QuixBugs** (2017)         | **Toy bugs** in classic algorithms (40 small programs, each in Python & Java). | **Correct output** for each bug’s sample test cases. (Usually one test per bug). | Trivial: 40 functions to run. Often used for quick demo of bug-fix techniques. | **Low** – Too simple and limited (no C++ at all, trivial algorithms). Not representative of real CI or complex code issues. |

**Table Key:** *Signals* = evaluation metrics for success. *Cost* = estimated difficulty or computational expense to integrate and run. *Fit* = relevance to our internal benchmark needs (C++ focus, CI/build context, multi-step agent use).

## Notes on Benchmark Usage and Fit

- **SWE-Bench (Multilingual)** stands out as a valuable benchmark to adopt. It directly includes C/C++ issue-fixing cases, which means our agent will face tasks requiring code navigation, editing, and compilation – just like our internal Boost CI scenarios. Its methodology (issue + tests) aligns with our evaluation style. Although the number of C/C++ tasks is modest, the cross-language aspect helps demonstrate our agent’s generality.

- **Defects4C** is a new but relevant benchmark, providing a wealth of C/C++ bugs with rigorous test-based evaluation. Adopting it will give us a defensible measure of how well our model repairs C/C++ code. It complements our internal dataset by adding diversity in bug types (including memory errors, which Boost might also encounter).

- **BugSwarm** offers real-world CI failures, the closest to what we aim to handle (e.g., failing builds, broken tests in a CI pipeline). Integrating even a subset (especially any C++ projects in it) would test our agent’s ability to read actual CI logs and handle environment-specific issues. It’s heavy to run, so we might be selective or use it to validate key capabilities rather than for frequent regression tests.

- **PolyBench** gives us insight into the agent’s **tool use** and long-horizon decision-making. Even though it doesn’t cover C++, successfully navigating a large JavaScript/Java project correlates with being able to handle large C++ codebases. The additional metrics (file and step precision/recall) are useful diagnostics: for instance, if our agent struggles with focusing on the right files in PolyBench, the same could happen in a big C++ project.

- **Common code-gen benchmarks** like HumanEval and MultiPL-E (C++) don’t mirror CI-fix tasks, but they are widely referenced. Reporting these scores ensures our benchmark results are **comparable and defensible**. For example, if our model’s HumanEval score is on par with GPT-4, but it still struggles on C++ CI tasks, that highlights the unique difficulty of those tasks. Conversely, a strong MultiPL-E C++ performance would indicate our model’s underlying knowledge of C++ is solid, and any CI-fix failures are due to complexity of integration, not basic syntax issues.

- Some benchmarks were deemed **not suitable** as primary evaluations: *QuixBugs* (too small/simple), *CodeXGLUE’s refinement* (text-match based, not execution-based), or *Bears* (Java-only, redundant with BugSwarm). We acknowledge them to show thoroughness, but they won’t add much new signal for our specific goals.

By grounding our internal efforts in these external benchmarks, we ensure that our evaluation is **comparable to existing work** and covers the crucial aspects of coding agents. The next step is to select a few of these benchmarks as **secondary validation** to run alongside our internal test suites, which we detail in a separate recommendations document.
