# MultiPL-E C++ and HumanEval: Methodology (Child Issue 5)

This document answers the research questions for the code-generation benchmark and describes translation, test adaptation, type handling, validation, and the replication plan for Boost/Clang.

## Research questions

### What is the official MultiPL-E repository/dataset location?

- **Dataset:** Hugging Face — [nuprl/MultiPL-E](https://huggingface.co/datasets/nuprl/MultiPL-E). Configurations follow `SRCDATA-LANG` (e.g. `humaneval-cpp` for C++).
- **Code:** GitHub — [nuprl/MultiPL-E](https://github.com/nuprl/MultiPL-E). Includes dataset builder, evaluation (pass_k.py, containerized execution), and language translators.

### How is MultiPL-E different from original HumanEval?

- **HumanEval** is OpenAI’s Python-only benchmark (164 problems): function signature + docstring (prompt), canonical solution, and unit tests. Evaluation is pass@k by running tests.
- **MultiPL-E** translates HumanEval (and MBPP) to many languages via “little compilers”: same problem set, translated prompts, function signatures, and tests. So MultiPL-E is **multi-language HumanEval** (and MBPP); C++ is one of the translated languages.

### What is the exact Hugging Face dataset path for C++ subset?

- **Dataset:** `nuprl/MultiPL-E`
- **Configuration:** `humaneval-cpp`
- **Load:** `load_dataset("nuprl/MultiPL-E", "humaneval-cpp", split="test")`

### How many problems are in the C++ subset?

- **161 problems** in the MultiPL-E C++ subset (HumanEval translated to C++). The original HumanEval has 164 Python problems; a few may be omitted or merged in translation. Our download script writes 161 to `data/cpp_problems.jsonl`.

### What is the format of each problem? (prompt, tests, canonical solution)

- **task_id:** e.g. `HumanEval_0_has_close_elements`
- **prompt:** C++ code: includes, comment (translated docstring), doctest-style examples, and function signature with opening `{`. The model completes the function body.
- **tests:** C++ code: closing `}`, `int main() { ... }` with `assert(candidate(...))` calls.
- **canonical_solution:** In the HF C++ config the canonical solution is often empty; the Python reference in `humaneval_python.jsonl` has it. For C++, evaluation uses model-generated completions only.

### How do I run evaluation? (pass@k metrics)

- **MultiPL-E upstream:** Generate completions (e.g. `automodel.py` / `automodel_vllm.py`), then run `evaluation/src/main.py` (optionally in the [multipl-e-evaluation](https://github.com/nuprl/MultiPL-E/pkgs/container/multipl-e-evaluation) container), then `pass_k.py` to compute pass@1, pass@10, pass@100.
- **Our harness:** `evaluation/evaluate_passk.py` — reads `cpp_problems.jsonl` and a completions JSONL (or directory), runs each completion in the sandbox (Docker), and writes pass@1 / pass@10 / pass@100 to `evaluate_passk.json`.

### What execution environment is needed for C++ test execution?

- **Compiler:** g++ (C++17). Our sandbox uses `gcc:12-bookworm-slim` with g++ and Python 3.
- **Execution:** Run the compiled binary with timeout; no network. The sandbox Docker image runs with `--network none` and mounts only the sandbox dir.

### Are there sanitized/unsafe execution options?

- **Sanitized:** Run inside a container (e.g. our `evaluation/sandbox/Dockerfile` or upstream `ghcr.io/nuprl/multipl-e-evaluation`) with no network and a timeout. Our `execute.py` uses subprocess timeout for compile and run.
- **Unsafe:** Running generated code on the host without isolation is not recommended. The `--no-docker` option in `evaluate_passk.py` runs `execute.py` locally for testing only.

### How do I handle compilation and execution safely?

- **Container:** Build our sandbox image: `docker build -t multiple-humaneval-sandbox:latest evaluation/sandbox`. Run with `docker run --rm --network none -v ... -i`.
- **Timeouts:** Compile timeout (e.g. 30s) and run timeout (e.g. 10s) in `execute.py` to avoid hangs.
- **No network:** `--network none` so generated code cannot access the network.

## Methodology documentation requirements

### Translation process (how were Python problems translated to C++?)

- MultiPL-E uses **language translators** (e.g. `humaneval_to_cpp.py` in `dataset_builder/`): they take the Python prompt (signature, docstring, doctests) and produce the C++ prompt (includes, comment, doctests as C++, function signature). The translator implements a `Translator` class with methods for prompt translation, test-suite prefix/suffix, and value-to-value mapping (e.g. Python list → C++ vector).

### Test adaptation (how were test cases converted?)

- **Doctests** in the docstring are translated to C++ comment form (e.g. `// >>> fn(...)` and expected result). The **unit tests** (assertions) are generated by the translator: `test_suite_prefix_lines`, `test_suite_suffix_lines`, and value generators so that Python literals become C++ literals (e.g. `gen_list` → `std::vector<...>` with correct types). The result is a C++ `int main()` that calls the candidate function and uses `assert(...)`.

### Type handling (how were dynamic types mapped to C++ types?)

- The translator maps Python types to C++ types: e.g. `list` → `std::vector`, `dict` → `std::map` or similar, `str` → `std::string`, `float`/`int`/`bool` to C++ primitives. Type inference or annotations from the Python side drive the C++ signature and test code. See the translator source in the MultiPL-E repo for exact mappings.

### Validation (how was translation correctness verified?)

- Upstream uses consistency checks and evaluation: run the canonical solution (when available) or model outputs against the translated tests; fix translator bugs when tests fail. The dataset has gone through multiple versions (changelog on HF) with bug fixes for various languages.

## Replication plan for Boost/Clang coding tasks

### Creating Boost-specific coding problems

- **Extract function signatures** from Boost documentation (e.g. algorithm, container APIs).
- **Create problems** in the form: “Implement `boost::algorithm::trim` with signature …” with a short spec and constraints.
- **Validation:** Use existing Boost tests (or a subset) as the test harness; our sandbox would need a Boost build environment instead of plain g++.
- **Coverage:** Containers, algorithms, utilities across libraries.

### Creating Clang-specific coding problems

- **Extract AST manipulation patterns** from LLVM/Clang docs and tests.
- **Create problems** such as: “Write a Clang AST visitor that …” with a clear contract.
- **Validation:** Use LLVM test infrastructure (lit, or a small driver that runs the visitor on a fixed input).
- **Coverage:** AST, diagnostics, code generation.

The **structure** is the same as HumanEval/MultiPL-E: task_id, prompt (spec + signature or entry point), tests (harness), and optional canonical_solution; the **environment** (Boost/Clang build, includes, test runner) is domain-specific.

## Summary

| Topic | Answer |
|-------|--------|
| MultiPL-E location | Hugging Face nuprl/MultiPL-E; GitHub nuprl/MultiPL-E |
| vs HumanEval | MultiPL-E = HumanEval (and MBPP) translated to many languages |
| C++ HF path | nuprl/MultiPL-E, config humaneval-cpp |
| C++ problem count | 161 |
| Format | task_id, prompt, tests, canonical_solution |
| pass@k | evaluate_passk.py + sandbox; upstream pass_k.py |
| C++ execution | g++, Docker, timeout, no network |
| Safe execution | Container + timeouts; avoid host execution |
| Replication Boost/Clang | Same prompt/tests structure; Boost/Clang-specific env and tests |
