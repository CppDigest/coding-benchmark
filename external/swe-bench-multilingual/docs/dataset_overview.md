# Dataset overview: SWE-Bench Multilingual

This document describes the structure of the SWE-Bench Multilingual dataset as used in this repo: where it comes from, where it is stored locally, and how to download and use it. For field-level schema see [schema.md](schema.md). For version pinning see [dataset_version.md](dataset_version.md). For mining methodology and replication plan see [methodology.md](methodology.md).

## Dataset identity and revision

- **Dataset ID:** `SWE-bench/SWE-bench_Multilingual` (Hugging Face).
- **Revision:** Pinned by commit hash; recorded in [dataset_version.md](dataset_version.md). Never use `main` without a pinned revision.

## Raw splits: names and formats

- **Split name:** The dataset currently exposes a single split, **`test`**.
- **Format:** Each split is stored as [Apache Parquet](https://parquet.apache.org/). One file per split: `test.parquet`.
- **Contents:** Each row is one benchmark instance (repo, base commit, problem statement, gold patch, test IDs, etc.). Field descriptions are in [schema.md](schema.md).

## Where files are stored locally

| Artifact | Location |
|----------|----------|
| **Manifest** | `external/swe-bench-multilingual/data/raw/manifest.json` |
| **Raw parquet splits** | `external/swe-bench-multilingual/data/raw/*.parquet` (e.g. `test.parquet`) |
| **C/C++ filtered subset** | `external/swe-bench-multilingual/data/cpp_issues.jsonl` (generated by `scripts/filter_cpp.py` from the raw parquet) |

The manifest is auto-generated by the download script and records `dataset_id`, `revision`, `downloaded_at_utc`, and `splits`. Use it to confirm which revision and splits you have.

## How to download (one command)

From the **repository root** (e.g. `coding-benchmark`):

```bash
python external/swe-bench-multilingual/scripts/download.py --revision <commit_hash>
```

Use the revision from [dataset_version.md](dataset_version.md). Output is written to `external/swe-bench-multilingual/data/raw/`: `manifest.json` and one `.parquet` file per split (e.g. `test.parquet`). Dependencies: `pip install datasets huggingface_hub pyarrow`.

## High-level evaluation and success criteria

- **SWE-Bench success:** An instance is *resolved* if, after applying the model’s patch to the base commit, the specified tests go from fail→pass and existing pass→pass tests do not regress. Evaluation can be execution-based (run tests) or, in standalone mode, patch comparison against the gold patch (see `evaluation/run_evaluation.py`).
- **Gold patches:** Stored in the `patch` (raw) / `gold_patch` (C/C++ JSONL) field as unified diffs; they are the reference fix for each issue.

## Replication plan

For how SWE-Bench mines issues and for a replication plan for Boost/Clang (and pointers to reproducing the methodology), see [methodology.md](methodology.md).
